{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990546f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Installation\\\\spark-3.3.2-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdcdecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "            SparkSession\n",
    "                .builder\n",
    "                .appName(\"SparkPartitionsApp\")\n",
    "    \n",
    "                # Local\n",
    "                .master(\"local[4]\")\n",
    "    \n",
    "                # Standalone/YARN    \n",
    "    \n",
    "                #.config(\"spark.cores.max\",            \"6\")\n",
    "    \n",
    "                #.config(\"spark.executor.memory\",      \"2g\")\n",
    "                #.config(\"spark.executor.cores\",       \"2\")\n",
    "    \n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    \n",
    "                .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051ee79f",
   "metadata": {},
   "source": [
    "### Check default parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0febe426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c0552",
   "metadata": {},
   "source": [
    "### 1. Partition settings while reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29088585",
   "metadata": {},
   "source": [
    "### Check partitions for a small dataset\n",
    "\n",
    "Spark finds optimal number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb5889be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 1\n",
      "Record Count = 265\n"
     ]
    }
   ],
   "source": [
    "# Read Taxi Zones data\n",
    "taxiZonesDF = (\n",
    "                  spark\n",
    "                    .read                    \n",
    "                    .option(\"inferSchema\", \"true\")\n",
    "                    .csv(\"C:\\SparkCourse\\DataFiles\\Raw\\TaxiZones.csv\")\n",
    "              )\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"    + str( taxiZonesDF.rdd.getNumPartitions() ))\n",
    "\n",
    "# Check number of records\n",
    "print(\"Record Count = \"  + str( taxiZonesDF.count() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05013349",
   "metadata": {},
   "source": [
    "### Check partitions for a large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fc8720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 4\n",
      "Record Count = 3675412\n"
     ]
    }
   ],
   "source": [
    "# Read Yellow Taxis data\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .option(\"header\", \"true\")    \n",
    "                    .option(\"inferSchema\", \"true\")    \n",
    "                    .csv(\"C:\\SparkCourse\\DataFiles\\Raw\\YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"    + str( yellowTaxiDF.rdd.getNumPartitions() ))\n",
    "\n",
    "# Check number of records\n",
    "print(\"Record Count = \"  + str( yellowTaxiDF.count() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91a6228",
   "metadata": {},
   "source": [
    "### Change maximum partition size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb27481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set( \"spark.sql.files.maxPartitionBytes\", \"64m\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106e60eb",
   "metadata": {},
   "source": [
    "### Check partitions for a large dataset\n",
    "\n",
    "With smaller max partition size (64 MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d537362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Parallelism = 4\n",
      "Partitions = 7\n"
     ]
    }
   ],
   "source": [
    "# Read Yellow Taxis data\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .option(\"header\", \"true\")    \n",
    "                    .option(\"inferSchema\", \"true\")    \n",
    "                    .csv(\"C:\\SparkCourse\\DataFiles\\Raw\\YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "# Check default parallelism\n",
    "print(\"Default Parallelism = \"  + str( sc.defaultParallelism ))\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"           + str( yellowTaxiDF.rdd.getNumPartitions() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c532794",
   "metadata": {},
   "source": [
    "### Create method to calculate DataFrame statistics\n",
    "\n",
    "Finds data for each partition <br/>\n",
    "Calculate count of records, and min & max values of a column across each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b43569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrameStats(dataFrame, columnName):\n",
    "\n",
    "    outputDF = (\n",
    "                    dataFrame\n",
    "\n",
    "                        # Get partition number for each record\n",
    "                        .withColumn(\"Partition Number\", spark_partition_id())\n",
    "        \n",
    "        \n",
    "                        # Group by partition, and calculate stats for a column\n",
    "                        .groupBy(\"Partition Number\")\n",
    "                        .agg(\n",
    "                                  count(\"*\").alias(\"Record Count\"),\n",
    "                                  min(columnName).alias(\"Min Column Value\"),\n",
    "                                  max(columnName).alias(\"Max Column Value\")\n",
    "                            )\n",
    "\n",
    "                        .orderBy(\"Partition Number\")\n",
    "               )\n",
    "\n",
    "    return outputDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f78f7",
   "metadata": {},
   "source": [
    "### Check stats for Yellow Taxis DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1af9484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|      531991|               1|             265|\n",
      "|               1|      531721|               1|             265|\n",
      "|               2|      531579|               1|             265|\n",
      "|               3|      531536|               1|             265|\n",
      "|               4|      531728|               1|             265|\n",
      "|               5|      531528|               1|             265|\n",
      "|               6|      485329|               1|             265|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getDataFrameStats( yellowTaxiDF, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c49b9",
   "metadata": {},
   "source": [
    "### 2. Partition settings while shuffling data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c163c6",
   "metadata": {},
   "source": [
    "### Check default number of shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e686a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.conf.get( \"spark.sql.shuffle.partitions\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf44f28",
   "metadata": {},
   "source": [
    "### Apply a shuffle operation and check DataFrame stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39440836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 200\n",
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|           1|             148|             148|\n",
      "|               1|           1|             243|             243|\n",
      "|               2|           1|              31|              31|\n",
      "|               3|           3|              85|             251|\n",
      "|               4|           1|              65|              65|\n",
      "|               5|           2|              53|             255|\n",
      "|               6|           1|             133|             133|\n",
      "|               7|           1|              78|              78|\n",
      "|              10|           2|             108|             155|\n",
      "|              11|           3|              34|             211|\n",
      "|              12|           3|             101|             126|\n",
      "|              13|           1|              81|              81|\n",
      "|              14|           3|              28|             210|\n",
      "|              18|           1|              76|              76|\n",
      "|              19|           2|              26|              27|\n",
      "|              21|           3|              44|             192|\n",
      "|              22|           1|             253|             253|\n",
      "|              23|           1|             236|             236|\n",
      "|              24|           1|              12|              12|\n",
      "|              25|           1|             223|             223|\n",
      "+----------------+------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data\n",
    "yellowTaxiGroupedDF = (\n",
    "                            yellowTaxiDF\n",
    "                                .groupBy(\"PULocationID\")\n",
    "                                .agg(sum(\"total_amount\"))\n",
    "                      )\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"  + str( yellowTaxiGroupedDF.rdd.getNumPartitions() ))\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( yellowTaxiGroupedDF, \"PULocationID\" ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4856f5",
   "metadata": {},
   "source": [
    "### Change default number of shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27d17b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set( \"spark.sql.shuffle.partitions\", 3 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a2fb8",
   "metadata": {},
   "source": [
    "### Apply a shuffle operation and check DataFrame stats\n",
    "\n",
    "After changing default shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0696a513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 3\n",
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|          88|               3|             263|\n",
      "|               1|          85|               1|             265|\n",
      "|               2|          87|              11|             264|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data\n",
    "yellowTaxiGroupedDF = (\n",
    "                            yellowTaxiDF\n",
    "                                .groupBy(\"PULocationID\")\n",
    "                                .agg(sum(\"total_amount\"))\n",
    "                      )\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"  + str( yellowTaxiGroupedDF.rdd.getNumPartitions() ))\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( yellowTaxiGroupedDF, \"PULocationID\" ).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8013e60d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd6923f0",
   "metadata": {},
   "source": [
    "### Check stats for Yellow Taxis DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2fe9153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|      531991|               1|             265|\n",
      "|               1|      531721|               1|             265|\n",
      "|               2|      531579|               1|             265|\n",
      "|               3|      531536|               1|             265|\n",
      "|               4|      531728|               1|             265|\n",
      "|               5|      531528|               1|             265|\n",
      "|               6|      485329|               1|             265|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "getDataFrameStats( yellowTaxiDF, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17ccc4",
   "metadata": {},
   "source": [
    "### 1. Repartition DataFrame: Round-Robin partitioning\n",
    "\n",
    "Create equal-sized partitions. Data is not co-located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f522fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|      262529|               1|             265|\n",
      "|               1|      262529|               1|             265|\n",
      "|               2|      262530|               1|             265|\n",
      "|               3|      262531|               1|             265|\n",
      "|               4|      262531|               1|             265|\n",
      "|               5|      262531|               1|             265|\n",
      "|               6|      262531|               1|             265|\n",
      "|               7|      262530|               1|             265|\n",
      "|               8|      262528|               1|             265|\n",
      "|               9|      262529|               1|             265|\n",
      "|              10|      262528|               1|             265|\n",
      "|              11|      262529|               1|             265|\n",
      "|              12|      262528|               1|             265|\n",
      "|              13|      262528|               1|             265|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repartionedDF1 = yellowTaxiDF.repartition( 14 )\n",
    "\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( repartionedDF1, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810aab8",
   "metadata": {},
   "source": [
    "### 2. Repartition DataFrame: Hash partitioning\n",
    "\n",
    "Co-locates the data. Partition sizes may not be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c575498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|     1246579|               3|             263|\n",
      "|               1|     1426282|               1|             265|\n",
      "|               2|     1002551|              11|             264|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql.shuffle.partitions = 3 (in previous clip)\n",
    "\n",
    "\n",
    "repartionedDF1 = yellowTaxiDF.repartition( \"PULocationID\" )\n",
    "\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( repartionedDF1, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f6f80",
   "metadata": {},
   "source": [
    "### 2. Repartition DataFrame: Hash partitioning & define number of partitions\n",
    "\n",
    "Co-locates the data. Partition sizes may not be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c2bae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|      405748|              12|             261|\n",
      "|               1|      493312|               8|             250|\n",
      "|               2|      168505|              18|             263|\n",
      "|               3|      207849|               9|             260|\n",
      "|               4|      191119|              52|             257|\n",
      "|               5|      202251|              43|             258|\n",
      "|               6|      259735|              13|             265|\n",
      "|               7|      386736|              24|             255|\n",
      "|               8|      142987|               4|             254|\n",
      "|               9|      406158|               1|             251|\n",
      "|              10|      108394|              28|             234|\n",
      "|              11|      231246|               3|             241|\n",
      "|              12|      203911|               5|             259|\n",
      "|              13|      267461|              11|             240|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repartionedDF1 = yellowTaxiDF.repartition( 14, \"PULocationID\" )\n",
    "\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( repartionedDF1, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3278aea",
   "metadata": {},
   "source": [
    "### 3. Repartition DataFrame: Range partitioning\n",
    "\n",
    "Sort and co-locates the data. Partition sizes may not be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43d8226c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|      266634|               1|              50|\n",
      "|               1|      238923|              51|              79|\n",
      "|               2|      290834|              80|             113|\n",
      "|               3|      258270|             114|             132|\n",
      "|               4|      218033|             133|             140|\n",
      "|               5|      261945|             141|             143|\n",
      "|               6|      286326|             144|             161|\n",
      "|               7|      304033|             162|             164|\n",
      "|               8|      243993|             165|             186|\n",
      "|               9|      244184|             187|             230|\n",
      "|              10|      359447|             231|             236|\n",
      "|              11|      176660|             237|             237|\n",
      "|              12|      317894|             238|             249|\n",
      "|              13|      208236|             250|             265|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repartionedDF1 = yellowTaxiDF.repartitionByRange( 14, \"PULocationID\" )\n",
    "\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( repartionedDF1, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69cadc1",
   "metadata": {},
   "source": [
    "### Coalesce DataFrame\n",
    "\n",
    "Reduces number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f804e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|     1595291|               1|             265|\n",
      "|               1|     2080121|               1|             265|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "coalescedDF = yellowTaxiDF.coalesce( 2 )\n",
    "\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats( coalescedDF, \"PULocationID\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0be6b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc39af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcde1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9042bd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1920490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e7d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e48301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca6ce4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee105b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
