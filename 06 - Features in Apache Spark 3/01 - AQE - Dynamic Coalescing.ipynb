{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77f90c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Installation\\\\spark-3.3.2-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1237d712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CrystalTalks-PC:4044\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AqeDynamicCoalescingApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21acec54280>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = (\n",
    "            SparkSession\n",
    "                .builder\n",
    "                .appName(\"AqeDynamicCoalescingApp\")\n",
    "    \n",
    "                .master(\"local[4]\")\n",
    "    \n",
    "                .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "\n",
    "    \n",
    "                # Disable Adaptive Query Execution framework\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    \n",
    "                .getOrCreate()\n",
    "        )\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413cbd7f",
   "metadata": {},
   "source": [
    "### Create method to calculate DataFrame statistics\n",
    "\n",
    "Finds data for each partition <br/>\n",
    "Calculate count of records, and min & max values of a column across each partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7639712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrameStats(dataFrame, columnName):\n",
    "\n",
    "    outputDF = (\n",
    "                    dataFrame\n",
    "                        .withColumn(\"Partition Number\", spark_partition_id())\n",
    "\n",
    "                        .groupBy(\"Partition Number\")    \n",
    "                        .agg(\n",
    "                                  count(\"*\").alias(\"Record Count\"),\n",
    "                                  min(columnName).alias(\"Min Column Value\"),\n",
    "                                  max(columnName).alias(\"Max Column Value\")\n",
    "                            )\n",
    "\n",
    "                        .orderBy(\"Partition Number\")\n",
    "               )\n",
    "\n",
    "    return outputDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2426c13a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 4\n"
     ]
    }
   ],
   "source": [
    "# Read Yellow Taxis data\n",
    "yellowTaxiDF = (\n",
    "                  spark\n",
    "                    .read\n",
    "                    .option(\"header\", \"true\")    \n",
    "                    .option(\"inferSchema\", \"true\")    \n",
    "                    .csv(\"C:\\SparkCourse\\DataFiles\\Raw\\YellowTaxis_202210.csv\")\n",
    "               )\n",
    "\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"    + str( yellowTaxiDF.rdd.getNumPartitions()) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b1e96",
   "metadata": {},
   "source": [
    "### Change default shuffle partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db36ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set( \"spark.sql.shuffle.partitions\", 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927be6d6",
   "metadata": {},
   "source": [
    "### Apply a wide transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7197623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+\n",
      "|VendorId|payment_type|   sum(total_amount)|\n",
      "+--------+------------+--------------------+\n",
      "|       1|           2|   3407067.990002185|\n",
      "|       2|           4|   1103.950000000002|\n",
      "|       1|           0|   704915.6500000019|\n",
      "|       1|           4|   73231.73999999865|\n",
      "|       1|           1|1.7903327639982704E7|\n",
      "|       6|           0|   279048.8899999997|\n",
      "|       2|           2|    9225232.37000579|\n",
      "|       1|           3|  186607.24000000514|\n",
      "|       2|           0|  2899635.7899999204|\n",
      "|       2|           1|4.7088665280070014E7|\n",
      "|       2|           3|   29.71999999999995|\n",
      "+--------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiGroupedDF = (\n",
    "                            yellowTaxiDF\n",
    "                                .groupBy(\"VendorId\", \"payment_type\")\n",
    "                                .agg(sum(\"total_amount\"))\n",
    "                      )\n",
    "\n",
    "yellowTaxiGroupedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b866bc9",
   "metadata": {},
   "source": [
    "### Check DataFrame partitions after shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7278435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 20\n",
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               1|           2|               1|               2|\n",
      "|               4|           1|               1|               1|\n",
      "|               5|           1|               1|               1|\n",
      "|               7|           1|               1|               1|\n",
      "|               8|           1|               6|               6|\n",
      "|               9|           3|               1|               2|\n",
      "|              17|           1|               2|               2|\n",
      "|              18|           1|               2|               2|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "print(\"Partitions = \"  + str( yellowTaxiGroupedDF.rdd.getNumPartitions() ))\n",
    "\n",
    "# Get partition stats\n",
    "getDataFrameStats(yellowTaxiGroupedDF, \"VendorId\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7ff2fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE - NOT SHOWN IN VIDEO\n",
    "(\n",
    "    yellowTaxiGroupedDF    \n",
    "            .write\n",
    "            \n",
    "            .option(\"header\", \"true\")\n",
    "            .option(\"dateFormat\", \"yyyy-MM-dd HH:mm:ss.S\")\n",
    "    \n",
    "            .mode(\"overwrite\")\n",
    "    \n",
    "            .csv(\"C:\\SparkCourse\\DataFiles\\Output\\AqeTest.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eaf697",
   "metadata": {},
   "source": [
    "### Enable Adaptive Query Execution - Dynamic Coalescing of Shuffle Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2401476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5911aaa8",
   "metadata": {},
   "source": [
    "### Apply a wide transformation and check DataFrame stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66868ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions = 1\n",
      "+----------------+------------+----------------+----------------+\n",
      "|Partition Number|Record Count|Min Column Value|Max Column Value|\n",
      "+----------------+------------+----------------+----------------+\n",
      "|               0|          11|               1|               6|\n",
      "+----------------+------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "yellowTaxiGroupedDF = (\n",
    "                            yellowTaxiDF\n",
    "                                .groupBy(\"VendorId\", \"payment_type\")\n",
    "                                .agg(sum(\"total_amount\"))\n",
    "                      )\n",
    "\n",
    "\n",
    "# Check number of partitions\n",
    "print(\"Partitions = \"  + str(yellowTaxiGroupedDF.rdd.getNumPartitions()))\n",
    "\n",
    "\n",
    "# Get DataFrame stats\n",
    "getDataFrameStats(yellowTaxiGroupedDF, \"VendorId\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936f0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d53af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d0f6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90925594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746340c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d0100",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99718136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44ea8bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d035a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd29a181",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216aaef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16885916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
